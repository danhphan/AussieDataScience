{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_collection_linkedin_job_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_collection_linkedin_job_list\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import datetime, os\n",
    "import re, time, requests\n",
    "import pandas as pd\n",
    "from parsel import Selector\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import *\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_driver(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path=config.PATH_CHROME_DRIVER,\n",
    "        options=chrome_options)\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "def get_job_info(selector):\n",
    "    title = selector.css('h3::text').get().strip()\n",
    "    published = selector.xpath(\"//time/@datetime\").get()\n",
    "    job_url = selector.xpath(\"//a/@href\").get()    \n",
    "    return [title, published, job_url]\n",
    "    \n",
    "def get_job_list(driver):\n",
    "    job_list = driver.find_elements_by_xpath(\"//ul[contains(@class,'jobs-search__results-list')]/li\")    \n",
    "    for job in job_list:\n",
    "        time.sleep(0.2)\n",
    "        selector = Selector(text=job.get_attribute(\"outerHTML\"))\n",
    "        yield get_job_info(selector)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the job list and append into a csv temporary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_temp_filename():\n",
    "    return config.LINKEDIN_JOBLIST_TEMP + \"_\" + today.strftime('%Y%m%d') + \".csv\"\n",
    "\n",
    "def crawl_new_job_list():\n",
    "    \"\"\"\n",
    "    This function crawl a job list into a temporary csv file\n",
    "    \"\"\"\n",
    "    today = datetime.datetime.now()\n",
    "    temp_file = get_temp_filename()\n",
    "\n",
    "    for url in config.LINKEDIN_URLS:\n",
    "        driver = get_driver(url)\n",
    "        job_list = list(get_job_list(driver))\n",
    "        jobs = pd.DataFrame(job_list, columns=['title','published', 'url'])\n",
    "        jobs.to_csv(temp_file, mode='a', index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_new_job_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the temp file and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "def clean_new_job_list():\n",
    "    temp_file = get_temp_filename()\n",
    "    jobs = pd.read_csv(temp_file)\n",
    "    # Remove row without published date and appropriate url\n",
    "    jobs = jobs[~(jobs.published == \"published\")]\n",
    "    # Remove duplicate rows\n",
    "    jobs.drop_duplicates(inplace=True)\n",
    "    jobs.published = pd.to_datetime(jobs.published)\n",
    "    jobs.to_csv(temp_file, index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_new_job_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append temp job list into a full job list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cut_date(n_days=3):\n",
    "    \"\"\"\n",
    "    This functin get a cut date by using latest date minus to n_days\n",
    "    \"\"\"\n",
    "    with open(config.LINKEDIN_JOBLIST_LATEST_DATE, 'r') as file:\n",
    "        latest_date = file.readline()\n",
    "        latest_date = datetime.datetime.strptime(latest_date, \"%Y%m%d\")      \n",
    "    if not latest_date:\n",
    "        latest_date = datetime.datetime.now()\n",
    "    cut_date = latest_date - datetime.timedelta(days=n_days)\n",
    "    return cut_date\n",
    "\n",
    "def update_latest_job_date(new_jobs):\n",
    "    \"\"\"\n",
    "    After having the new jobs list, the latest date is stored in a text file\n",
    "    \"\"\"\n",
    "    latest_date = list(new_jobs.published.sort_values(ascending=False))[0]\n",
    "    latest_date = latest_date.strftime(\"%Y%m%d\")\n",
    "    with open(config.LINKEDIN_JOBLIST_LATEST_DATE, 'w') as file:\n",
    "        file.write(latest_date)\n",
    "        \n",
    "def in_full_jobs_list(job):\n",
    "    \"\"\"\n",
    "    This function check if a job is already in the full jobs list\n",
    "    \"\"\"\n",
    "    jobs_file = config.LINKEDIN_JOBLIST\n",
    "    if os.path.isfile(jobs_file):\n",
    "        full_jobs = pd.read_csv(jobs_file)\n",
    "        if job.url in full_jobs.url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_temp_jobs():\n",
    "    temp_file = get_temp_filename()\n",
    "    jobs = pd.read_csv(temp_file)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_new_jobs():\n",
    "    # Only get the recent jobs after a cut date\n",
    "    jobs = get_temp_jobs()\n",
    "    cut_date = get_cut_date()    \n",
    "    jobs = jobs[jobs.published > cut_date]\n",
    "    # Check of the job is alreay in the full_job_list, if not, append it into new_jobs list\n",
    "    new_jobs = []\n",
    "    for index, job in jobs.iterrows():\n",
    "        if not in_full_jobs_list(job):\n",
    "            new_jobs.append(job)\n",
    "    # Convert to a data frame, and add \"collected\" column\n",
    "    new_jobs = pd.DataFrame(new_jobs)\n",
    "    new_jobs[\"collected\"] = 0\n",
    "    # Update the latest job date into a text file\n",
    "    update_latest_job_date(new_jobs)\n",
    "    return new_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def append_to_full_jobs():\n",
    "    jobs_file = config.LINKEDIN_JOBLIST\n",
    "    new_jobs = filter_new_jobs()\n",
    "    if os.path.isfile(jobs_file):\n",
    "        jobs_full = pd.read_csv(jobs_file)\n",
    "        jobs_full = jobs_full.append(new_jobs)\n",
    "    else:\n",
    "        jobs_full = new_jobs\n",
    "\n",
    "    jobs_full.to_csv(jobs_file, index=False)\n",
    "    print(\"Success append into the full jobs list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append_to_full_jobs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
